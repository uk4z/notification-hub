{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pELB1Jw7OqbB"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "This is a frame of the `main` notebook. It contains code to set up the environment and load the LLama pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjW20GuBz-O"
      },
      "source": [
        "# Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models and datasets.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU.\n",
        "- `rouge_score`: Allows us to get the ROUGE metric on Python.\n",
        "- `datasets`: Allows us to load datasets from the transformer library.\n",
        "- `py7zr`: Required in order to use the SAMSum corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktn8OG5rPgcl"
      },
      "outputs": [],
      "source": [
        "!pip install requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzONthxzCqXz"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVchIqEQCxev"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: Traceback (most recent call last):\n",
            "  File \"/opt/mamba/bin/huggingface-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/opt/mamba/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\n",
            "    service.run()\n",
            "  File \"/opt/mamba/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
            "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
            "  File \"/opt/mamba/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 113, in login\n",
            "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
            "  File \"/opt/mamba/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 189, in interpreter_login\n",
            "    token = getpass(\"Token: \")\n",
            "  File \"/opt/mamba/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "  File \"/opt/mamba/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "  File \"/opt/mamba/lib/python3.10/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ocmtwn8yC70C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "linogaliana\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWyorjJG6pd"
      },
      "source": [
        "#I. Creation of the work environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OEEaESgsAlI"
      },
      "source": [
        "# I.1. Loading Dataset\n",
        "\n",
        "We are going to analyze the dataset and see how we could use it to best fit our purpose.\n",
        "\n",
        "First, let's load the dataset and see how it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OfQkbJbzsIYH"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61cada1d40bb41b2a61c7a3a7438aa8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/6.06M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edbb884b27514933ba64a1c59a4ca0fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/347k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba97588e69f247768053586d6d19ede3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/335k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "738a0bcb5aa54ef8a66bceab65c48fe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03a67358905249b7b576dc07c5d1fa8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52afdfc87894461fa86a024418871778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3HoqZDaFJZW"
      },
      "source": [
        "Now we know that we have three subdatasets: `train`, `test`, `validation`.\n",
        "\n",
        "It will come in handy to train and evaluate the fine-tuned model but let's look a little bit further and see how a row looks like.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tgBifOqC8DJ",
        "outputId": "0e5b3dab-3bd3-4f06-c5ea-e00972a25894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row: {'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'} \n",
            "\n",
            "Amanda: I baked  cookies. Do you want some?\n",
            "Jerry: Sure!\n",
            "Amanda: I'll bring you tomorrow :-) \n",
            "\n",
            ">>  Amanda baked cookies and will bring Jerry some tomorrow. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_row(dataset):\n",
        "  \"\"\"\n",
        "  Show the first row of the dataset.\n",
        "\n",
        "  Parameters:\n",
        "        dataset (Dataset): The dataset which will be used to train the AI.\n",
        "\n",
        "  Returns:\n",
        "      None: Prints the row.\n",
        "  \"\"\"\n",
        "  sample = dataset['train'].select(range(1))\n",
        "  for line in sample:\n",
        "    print('Row:',  line, '\\n')\n",
        "    print(line['dialogue'], '\\n')\n",
        "    print('>> ', line['summary'], '\\n')\n",
        "\n",
        "show_row(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MvGkwqG_QC"
      },
      "source": [
        "The summary is a third-person utterance. However we want to train our model to summarize the message in the first-person to make it more conversational.\n",
        "\n",
        "Also the corpus contains around 25% of dialogues with more than two characters in it. We want to get rid of them because it will be harder to transform it to a first-person perspective with an AI model.\n",
        "\n",
        "We will be using Meta's model `LLama2` as it is a top-notch open source model handling text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3IKEkUpDLjN"
      },
      "source": [
        "# I.2. Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wBsFUVqHDNsg"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    287\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    390\u001b[0m         path_or_repo_id,\n\u001b[1;32m    391\u001b[0m         filename,\n\u001b[1;32m    392\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    393\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    394\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    395\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    396\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    397\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    398\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    399\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    400\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    401\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[39m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1239\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1240\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1241\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1242\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1243\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1244\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1245\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1246\u001b[0m     )\n\u001b[1;32m   1247\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1632\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1633\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1634\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1635\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1636\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1637\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1638\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    386\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    387\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    388\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:302\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    299\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepoNotFound\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    305\u001b[0m     response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m\n\u001b[1;32m    306\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mrequest \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[39m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-6597c44c-45b34d91500639ea1f246a01;c9516ef7-b869-42bc-910b-07717c4a5c9e)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json.\nYour request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m/home/onyxia/work/notification-hub/setup.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://user-lgaliana-538549-0.user.lab.sspcloud.fr/home/onyxia/work/notification-hub/setup.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://user-lgaliana-538549-0.user.lab.sspcloud.fr/home/onyxia/work/notification-hub/setup.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-lgaliana-538549-0.user.lab.sspcloud.fr/home/onyxia/work/notification-hub/setup.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:737\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    736\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    739\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:569\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m    568\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    570\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    571\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    572\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    573\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    574\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    575\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    576\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    577\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    578\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    579\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    580\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    581\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    582\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transformers/utils/hub.py:404\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    390\u001b[0m         path_or_repo_id,\n\u001b[1;32m    391\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`."
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ne-y66FDZOK"
      },
      "source": [
        "# I.3. Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ23SHtxDTQw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import accelerate\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JAvaUF4MVts"
      },
      "outputs": [],
      "source": [
        "def get_llama_response(prompts):\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompts,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "\n",
        "    return [sequence[0]['generated_text'] for sequence in sequences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KylHREcUuQI"
      },
      "source": [
        "We will be working with the following prompt:\n",
        "\n",
        "*Transform the following third-person sentence into first-person. Replace `Name1`'s pronouns and possessive determiners with first-person counterparts. Replace `Name2`'s pronouns and possessive determiners with second-person counterparts.  Adjust verb forms accordingly. Write the transformed sentence only and write this mention at the begining: 'Answer:'. '`Summary`'*\n",
        "\n",
        "Let's try it out with the example above.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geMiektTWa8Y"
      },
      "outputs": [],
      "source": [
        "prompt = \"Transform the following third-person sentence into first-person. Replace Amanda's pronouns and possessive determiners with first-person counterparts. Replace Jerry's pronouns and possessive determiners with second-person counterparts. Adjust verb forms accordingly. Write the transformed sentence only and write this mention at the begining: 'Answer:'. 'Amanda baked cookies and will bring Jerry some tomorrow. '\"\n",
        "responses = get_llama_response([prompt])\n",
        "\n",
        "print(responses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOOR-ADDYzCF"
      },
      "source": [
        "You might not get a satisfying response on your first attempt but if you retry a few attemps you should get something like this:\n",
        "\n",
        "*Answer: I baked cookies and will bring you some tomorrow.*\n",
        "\n",
        "Here the LLama's response is satisfying. However we are going to introduce a metric later in order to evaluate the performance of the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
